---
- hosts: localhost
  pre_tasks:
    - name: Include variables role
      ansible.builtin.include_role:
        name: variables
        
    - name: Login to Uptime Kuma
      lucasheld.uptime_kuma.login:
        api_url: "{{ uptime_kuma_api_url }}"
        api_username: "{{ UPTIMEKUMA_USER }}"
        api_password: "{{ UPTIMEKUMA_PASSWORD }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
      register: login_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: login_result is success

    - name: Set API token as fact
      set_fact:
        uptime_kuma_api_token: "{{ login_result.token }}"

    - name: Debug login result
      debug:
        var: login_result
      when: debug | bool and debug_level == 'info'

  roles:
    - variables

  vars:
    uptime_kuma_api_url: "https://uptime.sauna.re"
    uptime_kuma_username: "{{ UPTIMEKUMA_USER }}"
    uptime_kuma_password: "{{ UPTIMEKUMA_PASSWORD }}"
    initial_setup: true
    delete_all: false
    debug: true  # Enable debug for troubleshooting
    debug_level: "info" # Level of debug output (info, verbose)
    retry_attempts: 3
    retry_delay: 5
    api_timeout: 30
    api_wait_events: 5
    api_ssl_verify: true  # Disable SSL verify fore using self-signed cert
    loop_pause: 2  # Configurable pause between loop iterations
    
    # Define tags that should exist in Uptime Kuma
    uptime_kuma_tags:
      - name: "manual"
        color: "#808080"  # Gray color

    # Proxy configuration defaults
    proxy_defaults:
      protocol: "socks5h"  # socks5h for DNS resolution through proxy
      port: "{{ socks5_proxy_port }}"  # Use the port from variables
      auth: true
      active: true
      default: false  # Whether to set as default proxy for new monitors
      applyExisting: false  # Whether to apply to existing monitors
      username: "{{ SOCKS5_PROXY_USER }}"
      password: "{{ SOCKS5_PROXY_PASSWORD }}"

    # Define subgroup names per host to avoid naming collisions
    kuma_subgroups:
      basic: "BASIC"
      http: "HTTP"
      containers: "CONTAINERS"

    endpoint_hosts: "{{ lookup('ansible.builtin.vars', 'endpoint_hosts') }}"

    # Define notifications that should exist in Uptime Kuma
    uptime_kuma_notifications:
      - name: "Pushover"
        type: "pushover"
        settings:
          pushoveruserkey: "{{ PUSHOVER_UPTIMEKUMA_USER_KEY }}"
          pushoverapptoken: "{{ PUSHOVER_UPTIMEKUMA_APPLICATION_TOKEN }}"
          pushovertitle: "Prometheus Offline"
          pushoverpriority: 2
          pushoverttl: 60
          isDefault: false
      
      - name: "HomeAssistant"
        type: "HomeAssistant"
        settings:
          homeAssistantUrl: "https://{{ endpoints.homeassistant.dmz }}"
          longLivedAccessToken: "{{ HOMEASSISTANT_UPTIMEKUMA_LONG_LIVED_ACCESS_TOKEN }}"
          isDefault: false

    # Common notification defaults
    notification_defaults:
      state: present
      applyExisting: false
      isDefault: false

  tasks:

    # ============================================================
    # Section: Host Playbooks Loading
    # ============================================================
    - name: Find all host playbooks
      find:
        paths: "{{ ansible_home }}/playbooks/hosts"
        patterns: "*.yml"
      register: host_playbooks

    - name: Load and parse host playbooks
      set_fact:
        host_playbook_contents: "{{ lookup('file', item.path) | from_yaml }}"
      loop: "{{ host_playbooks.files }}"
      register: loaded_playbooks

    - name: Debug loaded host playbooks
      debug:
        msg: "Host {{ item.item.path | basename | splitext | first }}: {{ item.ansible_facts.host_playbook_contents }}"
      loop: "{{ loaded_playbooks.results }}"
      when: debug | bool and debug_level == 'verbose'

    # ============================================================
    # Section: Initial Settings
    # ============================================================
    - name: Set Uptime Kuma to trust reverse proxy headers
      lucasheld.uptime_kuma.settings:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
        trustProxy: yes
      register: trust_reverse_proxy_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: not trust_reverse_proxy_result.failed
      when: initial_setup | bool

    # ============================================================
    # Section: Monitor Cleanup (if delete_all is true)
    # ============================================================
    - name: Retrieve all monitors
      lucasheld.uptime_kuma.monitor_info:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      register: monitor_info_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: monitor_info_result is success
      when: delete_all

    - name: Delete all monitors if delete_all is true
      lucasheld.uptime_kuma.monitor:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        type: "{{ item.type }}"
        state: absent
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ monitor_info_result.monitors }}"
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: monitor_result is success
      when: delete_all

    # ============================================================
    # Section: Docker Hosts Management
    # ============================================================
    - name: Extract active docker services per host
      set_fact:
        active_docker_services: >-
          {%- set result = {} -%}
          {%- for playbook in loaded_playbooks.results -%}
            {%- set host = playbook.item.path | basename | splitext | first -%}
            {%- for task in playbook.ansible_facts.host_playbook_contents -%}
              {%- if task.roles is defined -%}
                {%- for role in task.roles -%}
                  {%- if role is mapping and role.role == '0docker_service' and role.vars is defined and role.vars.docker_service_roles is defined -%}
                    {%- set docker_roles = role.vars.docker_service_roles | default([]) -%}
                    {%- if docker_roles is not none -%}
                      {%- set _ = result.update({host: docker_roles}) -%}
                    {%- else -%}
                      {%- set _ = result.update({host: []}) -%}
                    {%- endif -%}
                  {%- endif -%}
                {%- endfor -%}
              {%- endif -%}
            {%- endfor -%}
          {%- endfor -%}
          {{ result }}
    
    - name: Debug active docker services per host
      debug:
        var: active_docker_services
      when: debug | bool and debug_level == 'info'

    - name: Gather information about existing Docker hosts in Uptime Kuma
      lucasheld.uptime_kuma.docker_host_info:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      register: docker_host_info_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: docker_host_info_result is success

    - name: Extract list of existing Docker host names
      set_fact:
        existing_docker_host_names: "{{ docker_host_info_result.docker_hosts | map(attribute='name') | list }}"

    - name: Debug existing Docker host names
      debug:
        var: existing_docker_host_names
      when: debug | bool and debug_level == 'info'

    - name: Add defined Docker hosts that do not exist
      lucasheld.uptime_kuma.docker_host:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.key | upper }}"
        dockerType: "{{ item.value.docker_connection.type }}"
        dockerDaemon: "{{ item.value.docker_connection.path }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ lookup('dict', endpoint_hosts) }}"
      when:
        - item.value.docker_connection is defined
        - (item.key | upper) not in existing_docker_host_names
      register: docker_host_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: not docker_host_result.failed

    - name: Remove Docker hosts that should no longer exist
      lucasheld.uptime_kuma.docker_host:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.key | upper }}"
        state: absent
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ lookup('dict', endpoint_hosts) }}"
      when:
        - item.value.docker_connection is not defined
        - (item.key | upper) in existing_docker_host_names
      register: docker_host_remove_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: not docker_host_remove_result.failed

    - name: Debug endpoint_hosts
      debug:
        var: endpoint_hosts
      when: debug | bool and debug_level == 'info'

    # ============================================================
    # Section: Proxy Management
    # ============================================================
    - name: Get information about existing proxies
      lucasheld.uptime_kuma.proxy_info:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      register: existing_proxies
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: existing_proxies is success

    - name: Debug existing proxies
      debug:
        var: existing_proxies
      when: debug | bool and debug_level == 'info'

    - name: Build list of required proxies from hosts with socks5proxy
      set_fact:
        required_proxies: >-
          {%- set proxies = [] -%}
          {%- for host_key, host_details in active_docker_services.items() -%}
            {%- if 'socks5proxy' in host_details -%}
              {%- for host, host_info in endpoint_hosts.items() if host | lower == host_key | lower -%}
                {%- for ip in host_info.ip | default([]) -%}
                  {%- set proxy = {
                    'host': ip,
                    'port': proxy_defaults.port,
                    'protocol': proxy_defaults.protocol,
                    'auth': proxy_defaults.auth,
                    'username': proxy_defaults.username,
                    'password': proxy_defaults.password,
                    'active': proxy_defaults.active,
                    'default': proxy_defaults.default,
                    'applyExisting': proxy_defaults.applyExisting
                  } -%}
                  {%- do proxies.append(proxy) -%}
                {%- endfor -%}
              {%- endfor -%}
            {%- endif -%}
          {%- endfor -%}
          {{ proxies }}

    - name: Debug required proxies
      debug:
        var: required_proxies
      when: debug | bool and debug_level == 'info'

    - name: Get proxy fingerprints
      set_fact:
        existing_proxy_fingerprints: >-
          {%- set fingerprints = [] -%}
          {%- for proxy in existing_proxies.proxies -%}
            {%- set fingerprint = proxy.host ~ ':' ~ proxy.port ~ ':' ~ proxy.protocol -%}
            {%- do fingerprints.append(fingerprint) -%}
          {%- endfor -%}
          {{ fingerprints }}
        required_proxy_fingerprints: >-
          {%- set fingerprints = [] -%}
          {%- for proxy in required_proxies -%}
            {%- set fingerprint = proxy.host ~ ':' ~ proxy.port ~ ':' ~ proxy.protocol -%}
            {%- do fingerprints.append(fingerprint) -%}
          {%- endfor -%}
          {{ fingerprints }}

    - name: Create missing proxies
      lucasheld.uptime_kuma.proxy:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        host: "{{ item.host }}"
        port: "{{ item.port }}"
        protocol: "{{ item.protocol }}"
        auth: "{{ item.auth }}"
        username: "{{ item.username }}"
        password: "{{ item.password }}"
        active: "{{ item.active }}"
        default: "{{ item.default }}"
        applyExisting: "{{ item.applyExisting }}"
        state: present
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ required_proxies }}"
      when: item.host ~ ':' ~ item.port ~ ':' ~ item.protocol not in existing_proxy_fingerprints
      register: proxy_create_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: proxy_create_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    - name: Remove obsolete proxies
      lucasheld.uptime_kuma.proxy:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        host: "{{ item.host }}"
        port: "{{ item.port }}"
        state: absent
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ existing_proxies.proxies }}"
      when: item.host ~ ':' ~ item.port ~ ':' ~ item.protocol not in required_proxy_fingerprints
      register: proxy_delete_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: proxy_delete_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    # ============================================================
    # Section: Notification Management
    # ============================================================
    - name: Get information about existing notifications
      lucasheld.uptime_kuma.notification_info:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      register: existing_notifications
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: existing_notifications is success

    - name: Debug existing notifications
      debug:
        var: existing_notifications
      when: debug | bool and debug_level == 'info'

    - name: Generate notification fingerprints
      set_fact:
        existing_notification_fingerprints: >-
          {%- set result = {} -%}
          {%- for notif in existing_notifications.notifications -%}
            {%- set fingerprint = {
              'name': notif.name | string,
              'type': notif.type | string,
              'isDefault': notif.isDefault | default(false) | bool,
              'active': notif.active | default(true) | bool,
              'applyExisting': notif.applyExisting | default(false) | bool,
              'pushoveruserkey': notif.pushoveruserkey | default('') | string,
              'pushoverapptoken': notif.pushoverapptoken | default('') | string,
              'pushovertitle': notif.pushovertitle | default('') | string,
              'pushoverpriority': (notif.pushoverpriority | default('')) | string,
              'pushoverttl': (notif.pushoverttl | default('')) | int,
              'homeAssistantUrl': notif.homeAssistantUrl | default('') | string,
              'longLivedAccessToken': notif.longLivedAccessToken | default('') | string
            } -%}
            {%- set _ = result.update({notif.name: fingerprint}) -%}
          {%- endfor -%}
          {{ result | to_json }}
        desired_notification_fingerprints: >-
          {%- set result = {} -%}
          {%- for notif in uptime_kuma_notifications -%}
            {%- set merged_settings = notification_defaults | combine(notif.settings) -%}
            {%- set fingerprint = {
              'name': notif.name | string,
              'type': notif.type | string,
              'isDefault': merged_settings.isDefault | default(false) | bool,
              'active': merged_settings.active | default(true) | bool,
              'applyExisting': merged_settings.applyExisting | default(false) | bool,
              'pushoveruserkey': merged_settings.pushoveruserkey | default('') | string,
              'pushoverapptoken': merged_settings.pushoverapptoken | default('') | string,
              'pushovertitle': merged_settings.pushovertitle | default('') | string,
              'pushoverpriority': (merged_settings.pushoverpriority | default('')) | string,
              'pushoverttl': (merged_settings.pushoverttl | default('')) | int,
              'homeAssistantUrl': merged_settings.homeAssistantUrl | default('') | string,
              'longLivedAccessToken': merged_settings.longLivedAccessToken | default('') | string
            } -%}
            {%- set _ = result.update({notif.name: fingerprint}) -%}
          {%- endfor -%}
          {{ result | to_json }}

    - name: Determine notifications that need updating
      set_fact:
        notifications_to_update: >-
          {%- set existing = existing_notification_fingerprints | from_json -%}
          {%- set desired = desired_notification_fingerprints | from_json -%}
          {%- set to_update = [] -%}
          {%- for notif in uptime_kuma_notifications -%}
            {%- if notif.name in existing and existing[notif.name] != desired[notif.name] -%}
              {%- set _ = to_update.append(notif) -%}
            {%- endif -%}
          {%- endfor -%}
          {{ to_update | to_json }}

    - name: Determine new notifications
      set_fact:
        notifications_to_create: >-
          {%- set existing = existing_notification_fingerprints | from_json -%}
          {%- set to_create = [] -%}
          {%- for notif in uptime_kuma_notifications -%}
            {%- if notif.name not in existing -%}
              {%- set _ = to_create.append(notif) -%}
            {%- endif -%}
          {%- endfor -%}
          {{ to_create | to_json }}

    - name: Debug notification changes
      debug:
        msg:
          - "Notifications to create: {{ notifications_to_create | from_json | map(attribute='name') | list }}"
          - "Notifications to update: {{ notifications_to_update | from_json | map(attribute='name') | list }}"
      when: debug | bool and debug_level == 'info'

    - name: Create or update notifications
      vars:
        merged_settings: "{{ notification_defaults | combine(item.settings) }}"
      lucasheld.uptime_kuma.notification:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        type: "{{ item.type }}"
        state: "{{ merged_settings.state }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
        applyExisting: "{{ merged_settings.applyExisting }}"
        isDefault: "{{ merged_settings.isDefault }}"
        pushoveruserkey: "{{ merged_settings.pushoveruserkey | default(omit) }}"
        pushoverapptoken: "{{ merged_settings.pushoverapptoken | default(omit) }}"
        pushovertitle: "{{ merged_settings.pushovertitle | default(omit) }}"
        pushoverpriority: "{{ merged_settings.pushoverpriority | default(omit) }}"
        pushoverttl: "{{ merged_settings.pushoverttl | default(omit) }}"
        homeAssistantUrl: "{{ merged_settings.homeAssistantUrl | default(omit) }}"
        longLivedAccessToken: "{{ merged_settings.longLivedAccessToken | default(omit) }}"
      loop: "{{ (notifications_to_create | from_json) + (notifications_to_update | from_json) }}"
      register: notification_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: notification_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    - name: Remove obsolete notifications
      lucasheld.uptime_kuma.notification:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        state: absent
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ existing_notifications.notifications }}"
      when: item.name not in (uptime_kuma_notifications | map(attribute='name') | list)
      register: notification_delete_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: notification_delete_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    # ============================================================
    # Section: Tag Management
    # ============================================================
    - name: Get information about existing tags
      lucasheld.uptime_kuma.tag_info:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      register: existing_tags
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: existing_tags is success

    - name: Debug existing tags
      debug:
        var: existing_tags
      when: debug | bool and debug_level == 'info'

    - name: Create list of existing tag names
      set_fact:
        existing_tag_names: "{{ existing_tags.tags | map(attribute='name') | list }}"

    - name: Create list of desired tag names
      set_fact:
        desired_tag_names: "{{ uptime_kuma_tags | map(attribute='name') | list }}"

    - name: Debug tag lists
      debug:
        msg:
          - "Existing tags: {{ existing_tag_names }}"
          - "Desired tags: {{ desired_tag_names }}"
      when: debug | bool and debug_level == 'info'

    - name: Create missing tags
      lucasheld.uptime_kuma.tag:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        color: "{{ item.color }}"
        state: present
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ uptime_kuma_tags }}"
      when: item.name not in existing_tag_names
      register: tag_create_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: tag_create_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    - name: Remove obsolete tags
      lucasheld.uptime_kuma.tag:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        state: absent
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ existing_tags.tags }}"
      when: item.name not in desired_tag_names
      register: tag_delete_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: tag_delete_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    # ============================================================
    # Section: Monitor List Building
    # ============================================================
    - name: Build full monitor list (groups, subgroups, and individual checks)
      set_fact:
        complete_monitor_list: >-
          {% set all = [] %}
          {%- for host_key, host_details in endpoint_hosts.items() %}
            {%- set parent_group = {
              "name": host_key | upper,
              "type": "group"
            } -%}
            {%- set _ = all.append(parent_group) -%}

            {%- set has_basic = host_details.uptimekuma_basic_monitoring | default(false) %}
            {%- set has_http = host_details.uptimekuma_http_monitoring | default(false) %}
            {%- set has_container = host_details.uptimekuma_container_monitoring | default(false) %}

            {%- set host_services = active_docker_services[host_key | lower] | default([]) -%}

            {%- if has_basic %}
              {%- set _ = all.append({
                "name": kuma_subgroups.basic ~ " (" ~ host_key | upper ~ ")",
                "type": "group",
                "parent_name": parent_group.name
              }) -%}

              {%- for ip in host_details.ip | default([]) %}
                {%- set _ = all.append({
                      "name": "PING (" ~ ip ~ ")",
                      "type": "ping",
                      "hostname": ip,
                      "parent_name": kuma_subgroups.basic ~ " (" ~ host_key | upper ~ ")"
                    }) -%}

                {%- if host_details.open_ports is defined %}
                  {%- for port in host_details.open_ports %}
                    {%- set _ = all.append({
                      "name": "PORT " ~ port | string ~ " (" ~ ip ~ ")",
                      "type": "port",
                      "hostname": ip,
                      "port": port,
                      "parent_name": kuma_subgroups.basic ~ " (" ~ host_key | upper ~ ")"
                    }) -%}
                  {%- endfor %}
                {%- endif %}

                {%- if host_details.dns is defined and host_details.dns %}
                  {%- set _ = all.append({
                    "name": "DNS (" ~ ip ~ ")",
                    "type": "dns",
                    "hostname": "google.com",
                    "dns_resolve_server": ip,
                    "parent_name": kuma_subgroups.basic ~ " (" ~ host_key | upper ~ ")"
                  }) -%}
                {%- endif %}
              {%- endfor %}
            {%- endif %}

            {%- if has_http %}
              {%- set _ = all.append({
                "name": kuma_subgroups.http ~ " (" ~ host_key | upper ~ ")",
                "type": "group",
                "parent_name": parent_group.name
              }) -%}

              {%- for service_name, service_details in endpoints.items() %}
                {%- if service_name in host_services %}
                  {%- if not (service_details.prometheus_exporter | default(false)) %}
                    {%- set default_settings = {
                      "type": "http",
                      "interval": 60,
                      "maxretries": 3,
                      "retryInterval": 30,
                      "verify_ssl": true,
                      "accepted_statuscodes": ["200-299"],
                      "method": "GET",
                      "monitoring_disabled": false
                    } %}
                    {%- set settings = service_details.uptimekuma_settings | default(default_settings) %}
                    {%- if not (settings.monitoring_disabled | default(false)) %}
                      {%- for sub_host, urlval in service_details.items() if service_details is mapping and sub_host == host_key %}
                        {%- if urlval is string %}
                          {%- set base_url = "https://" ~ urlval %}
                          {%- set url_path = settings.url_path | default('') %}
                          {%- set full_url = base_url ~ url_path %}
                          
                          {%- set monitor_settings = {
                            "name": (service_name | capitalize) ~ " HTTP (" ~ (host_key | upper) ~ ")",
                            "type": settings.type | default("http"),
                            "url": full_url,
                            "parent_name": kuma_subgroups.http ~ " (" ~ host_key | upper ~ ")",
                            "interval": settings.interval | default(60),
                            "maxretries": settings.maxretries | default(3),
                            "retryInterval": settings.retryInterval | default(30),
                            "upsideDown": settings.upside_down | default(false),
                            "description": settings.description | default(omit),
                            "method": settings.method | default("GET"),
                            "body": settings.body | default(omit),
                            "headers": settings.headers | default(omit),
                            "httpBodyEncoding": settings.body_encoding | default(omit),
                            "timeout": settings.advanced_http.timeout | default(omit),
                            "maxredirects": settings.advanced_http.max_redirects | default(omit),
                            "authMethod": settings.authentication.type | default(omit),
                            "basic_auth_user": settings.authentication.username | default(omit),
                            "basic_auth_pass": settings.authentication.password | default(omit),
                            "authDomain": settings.authentication.domain | default(omit),
                            "authWorkstation": settings.authentication.workstation | default(omit),
                            "ignoreTls": not (settings.verify_ssl | default(true)),
                            "tlsCa": settings.tls.ca | default(omit),
                            "tlsCert": settings.tls.cert | default(omit),
                            "tlsKey": settings.tls.key | default(omit),
                            "expiryNotification": settings.tls.expiry_notification | default(false),
                            "tlsExpiryNotifyDays": settings.tls.expiry_days | default(omit),
                            "keyword": (settings.keyword_monitoring.keyword if settings.keyword_monitoring.enabled | default(false)) | default(omit),
                            "invertKeyword": settings.keyword_monitoring.invert | default(false),
                            "accepted_statuscodes": settings.accepted_statuscodes | default(["200-299"]),
                            "proxyId": settings.proxy.name | default(omit),
                            "tags": settings.tags | default([]),
                            "notificationIDList": settings.notifications | default([])
                          } -%}
                          {%- set _ = all.append(monitor_settings) -%}
                        {%- endif %}
                      {%- endfor %}
                    {%- endif %}
                  {%- endif %}
                {%- endif %}
              {%- endfor %}
            {%- endif %}

            {%- if has_container %}
              {%- set _ = all.append({
                "name": kuma_subgroups.containers ~ " (" ~ host_key | upper ~ ")",
                "type": "group",
                "parent_name": parent_group.name
              }) -%}

              {%- for service_name, service_details in endpoints.items() %}
                {%- if service_name in host_services %}
                  {%- if service_details.uptimekuma_settings.containers is defined %}
                    {%- for container_name, container_defs in service_details.uptimekuma_settings.containers.items() %}
                      {%- if container_defs.docker_connection_host | lower == host_key | lower %}
                        {%- set container_settings = container_defs.container_settings | default({}) %}
                        {%- set monitor_settings = {
                          "name": (container_name | capitalize) ~ " CONTAINER (" ~ (host_key | upper) ~ ")",
                          "type": "docker",
                          "docker_container": container_name,
                          "docker_host_name": host_key | upper,
                          "parent_name": kuma_subgroups.containers ~ " (" ~ host_key | upper ~ ")",
                          "interval": container_settings.interval | default(60),
                          "maxretries": container_settings.retries | default(3),
                          "notificationIDList": container_settings.notifications | default([])
                        } -%}
                        {%- set _ = all.append(monitor_settings) -%}
                      {%- endif %}
                    {%- endfor %}
                  {%- endif %}
                {%- endif %}
              {%- endfor %}
            {%- endif %}
          {%- endfor %}

          {{ all | to_json }}

    - name: Debug final list of all groups/monitors to create
      debug:
        var: complete_monitor_list
      when: debug | bool and debug_level == 'info'

    - name: Convert that JSON string into real lists/dicts
      set_fact:
        complete_monitor_list: "{{ complete_monitor_list | from_json }}"

    - name: Separate groups from monitors
      set_fact:
        group_list: "{{ complete_monitor_list | selectattr('type', 'equalto', 'group') | list }}"
        monitor_list: "{{ complete_monitor_list | rejectattr('type', 'equalto', 'group') | list }}"

    - name: Debug group list
      debug:
        var: group_list
      when: debug | bool and debug_level == 'info'

    - name: Debug monitor list
      debug:
        var: monitor_list
      when: debug | bool and debug_level == 'info'

    - name: Get all monitors currently set up in Uptime Kuma
      lucasheld.uptime_kuma.monitor_info:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        api_timeout: "{{ api_timeout }}"
        api_wait_events: "{{ api_wait_events }}"
        api_ssl_verify: "{{ api_ssl_verify }}"
      register: existing_monitors
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: existing_monitors is success

    - name: Debug all monitors currently set up in Uptime Kuma
      debug:
        var: existing_monitors
      when: debug | bool and debug_level == 'verbose'

    # ============================================================
    # Section: Monitor Comparison
    # ============================================================
    - name: Find which of our newly-defined monitors already exist
      set_fact:
        common_entries: >-
          {%- set result = [] -%}
          {%- for new_monitor in complete_monitor_list -%}
            {%- for existing_monitor in existing_monitors.monitors -%}
              {%- if new_monitor.name == existing_monitor.name and new_monitor.type == existing_monitor.type -%}
                {%- set _ = result.append(new_monitor) -%}
              {%- endif -%}
            {%- endfor -%}
          {%- endfor -%}
          {{ result | to_json }}

    - name: Convert common_entries from JSON string
      set_fact:
        common_entries: "{{ common_entries | from_json }}"

    - name: Generate common_keys for existing monitors/groups
      set_fact:
        common_keys: >-
          {%- set keys = [] -%}
          {%- for monitor in existing_monitors.monitors -%}
            {%- if monitor.type == 'group' -%}
              {%- set _ = keys.append(monitor.name ~ '|' ~ monitor.type) -%}
            {%- else -%}
              {%- set _ = keys.append(monitor.name ~ '|' ~ monitor.type ~ '|' ~ (monitor.hostname|default('')) ~ '|' ~ (monitor.port|default('')) ~ '|' ~ (monitor.url|default('')) ~ '|' ~ (monitor.dns_resolve_server|default('')) ~ '|' ~ (monitor.docker_container|default('')) ~ '|' ~ (monitor.docker_host_name|default(''))) -%}
            {%- endif -%}
          {%- endfor -%}
          {{ keys }}

    - name: Debug common keys
      debug:
        var: common_keys
      when: debug | bool and debug_level == 'info'

    - name: Debug common entries that were found
      debug:
        var: common_entries
      when: debug | bool and debug_level == 'info'

    - name: Determine missing groups
      set_fact:
        missing_groups: >
          {%- set missing = [] -%}
          {%- for g in group_list -%}
            {%- if (g.name ~ '|' ~ g.type) not in common_keys -%}
              {%- do missing.append(g) -%}
            {%- endif -%}
          {%- endfor -%}
          {{ missing }}

    - name: Generate monitor keys for comparison
      set_fact:
        current_monitor_keys: >-
          {%- set result = {} -%}
          {%- for monitor in complete_monitor_list -%}
            {%- if monitor.type != 'group' -%}
              {%- set key_parts = [] -%}
              {%- set _ = key_parts.append(monitor.name) -%}
              {%- set _ = key_parts.append(monitor.type) -%}
              {%- if monitor.type == 'http' -%}
                {%- set _ = key_parts.append(monitor.url | default('')) -%}
              {%- elif monitor.type == 'ping' -%}
                {%- set _ = key_parts.append(monitor.hostname | default('')) -%}
              {%- elif monitor.type == 'port' -%}
                {%- set _ = key_parts.append(monitor.hostname | default('')) -%}
                {%- set _ = key_parts.append(monitor.port | string | default('')) -%}
              {%- elif monitor.type == 'dns' -%}
                {%- set _ = key_parts.append(monitor.hostname | default('')) -%}
                {%- set _ = key_parts.append(monitor.dns_resolve_server | default('')) -%}
              {%- elif monitor.type == 'docker' -%}
                {%- set _ = key_parts.append(monitor.docker_container | default('')) -%}
                {%- set _ = key_parts.append(monitor.docker_host_name | default('')) -%}
              {%- endif -%}
              {%- set _ = result.update({(key_parts | join('|')): monitor}) -%}
            {%- endif -%}
          {%- endfor -%}
          {{ result | to_json }}

    - name: Generate existing monitor keys
      set_fact:
        existing_monitor_keys: >-
          {%- set result = {} -%}
          {%- for monitor in existing_monitors.monitors -%}
            {%- if monitor.type != 'group' -%}
              {%- set key_parts = [] -%}
              {%- set _ = key_parts.append(monitor.name) -%}
              {%- set _ = key_parts.append(monitor.type) -%}
              {%- if monitor.type == 'http' -%}
                {%- set _ = key_parts.append(monitor.url | default('')) -%}
              {%- elif monitor.type == 'ping' -%}
                {%- set _ = key_parts.append(monitor.hostname | default('')) -%}
              {%- elif monitor.type == 'port' -%}
                {%- set _ = key_parts.append(monitor.hostname | default('')) -%}
                {%- set _ = key_parts.append(monitor.port | string | default('')) -%}
              {%- elif monitor.type == 'dns' -%}
                {%- set _ = key_parts.append(monitor.hostname | default('')) -%}
                {%- set _ = key_parts.append(monitor.dns_resolve_server | default('')) -%}
              {%- elif monitor.type == 'docker' -%}
                {%- set _ = key_parts.append(monitor.docker_container | default('')) -%}
                {%- set _ = key_parts.append(monitor.docker_host_name | default('')) -%}
              {%- endif -%}
              {%- set _ = result.update({(key_parts | join('|')): monitor}) -%}
            {%- endif -%}
          {%- endfor -%}
          {{ result | to_json }}

    - name: Determine missing monitors
      set_fact:
        missing_monitors: >-
          {%- set current_keys = (current_monitor_keys | from_json).keys() | list -%}
          {%- set existing_keys = (existing_monitor_keys | from_json).keys() | list -%}
          {%- set missing = [] -%}
          {%- for key in current_keys -%}
            {%- if key not in existing_keys -%}
              {%- set monitor = (current_monitor_keys | from_json)[key] -%}
              {%- set _ = missing.append(monitor) -%}
            {%- endif -%}
          {%- endfor -%}
          {{ missing }}

    - name: Debug missing groups
      debug:
        var: missing_groups
      when: debug | bool and debug_level == 'info'

    - name: Debug missing monitors
      debug:
        var: missing_monitors
      when: debug | bool and debug_level == 'info'

    - name: Determine obsolete monitors
      set_fact:
        obsolete_monitors: >-
          {%- set current_keys = (current_monitor_keys | from_json).keys() | list -%}
          {%- set existing_keys = (existing_monitor_keys | from_json).keys() | list -%}
          {%- set obsolete = [] -%}
          {%- for key in existing_keys -%}
            {%- if key not in current_keys -%}
              {%- set monitor = (existing_monitor_keys | from_json)[key] -%}
              {%- if 'manual' not in (monitor.tags | map(attribute='name') | list) -%}
                {%- set _ = obsolete.append(monitor) -%}
              {%- endif -%}
            {%- endif -%}
          {%- endfor -%}
          {{ obsolete | to_json }}

    - name: Debug obsolete monitors
      debug:
        var: obsolete_monitors | from_json
      when: debug | bool and debug_level == 'info'

    # ============================================================
    # Section: Group Creation
    # ============================================================
    - name: Create missing groups
      lucasheld.uptime_kuma.monitor:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        type: "{{ item.type }}"
        parent_name: "{{ item.parent_name | default(omit) }}"
        interval: 60
        maxretries: 5
        state: present
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ missing_groups }}"
      register: group_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: group_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    # ============================================================
    # Section: Monitor Creation
    # ============================================================
    - name: Create missing monitors
      lucasheld.uptime_kuma.monitor:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        type: "{{ item.type }}"
        parent_name: "{{ item.parent_name | default(omit) }}"
        hostname: "{{ item.hostname | default(omit) }}"
        port: "{{ item.port | default(omit) }}"
        url: "{{ item.url | default(omit) }}"
        dns_resolve_server: "{{ item.dns_resolve_server | default(omit) }}"
        docker_container: "{{ item.docker_container | default(omit) }}"
        docker_host_name: "{{ item.docker_host_name | default(omit) }}"
        interval: 60
        maxretries: 5
        state: present
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ missing_monitors }}"
      register: monitor_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: monitor_result is success
      loop_control:
        pause: "{{ loop_pause }}"

    # ============================================================
    # Section: Monitor Deletion
    # ============================================================
    - name: Delete obsolete monitors
      lucasheld.uptime_kuma.monitor:
        api_url: "{{ uptime_kuma_api_url }}"
        api_token: "{{ uptime_kuma_api_token }}"
        name: "{{ item.name }}"
        type: "{{ item.type }}"
        state: absent
        api_ssl_verify: "{{ api_ssl_verify }}"
      loop: "{{ obsolete_monitors | from_json }}"
      register: delete_result
      retries: "{{ retry_attempts }}"
      delay: "{{ retry_delay }}"
      until: delete_result is success
      loop_control:
        pause: "{{ loop_pause }}"